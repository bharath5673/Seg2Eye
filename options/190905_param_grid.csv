190905_size256_bs1_lr.0002_l1_0_l2_15_w8___spadeStyleGen_cmseq_ns7_SAMmax_SASw

== Current top model (score: 67)
Best parameters so far (model 190905_size256_bs1_lr.0002_l1_0_l2_15_w8___spadeStyleGen_cmseq_ns7_SAMmax_SASw):
load_size 256
lr .0002
lambda_l1 0
lambda_l2 15
w_dim 8
combine_mode seq
input_ns 7
style_aggr_method max

== Grid
I created two grids, a smaller and a bigger one. Please feel free to adjust the parameters if you want.

Big grid:

load_size,256,512
lr,0.0001,0.0002,0.0004,0.0008,0.0016
lambda_l1,10,15
lambda_l2,10,15
w_dim,8,16,32,64
weight_decay,0,0.001
combine_mode,add,seq
style_aggr_method,mean,max
beta1,0.5,0


Small grid:

load_size,256,512
lr,0.0001,0.0002,0.0004
lambda_l1,10,15
lambda_l2,10,15
w_dim,8,32


input_ns: as many as possible. I could fit 7 in my GTX with 12GB.
load_size should be equal to crop_size

== Optimizer
Have you heard of RAdam? I trained some models with Adam and Radam, but got similar results. However, I did not tune the learning rate. Do you believe we should give it a try?
https://arxiv.org/abs/1908.03265
Currently, we use one learning rate and have Adam handle the rest. However, oftentimes people have a decay (e.g. linear) after some epochs.
We can set this with `--niter` and `--niter_decay` (number of epoches to train and then number of epochs to linearly decay). What is your intuition here, Shalini?

